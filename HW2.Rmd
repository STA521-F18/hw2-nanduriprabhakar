---
title: "HW2 STA521 Fall18"
author: 'Prabhakar Nanduri || Netid: pnn2 || github username: nanduriprabhakar '
date: "September 23, 2018"
output:
  pdf_document: default
  html_document:
    df_print: paged
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(dplyr)
library(GGally)
library(ggplot2)
```

## Exploratory Data Analysis

```{r data, echo=FALSE}
library(alr3)
data(UN3, package="alr3")
help(UN3) 
library(car)
```

## Summary of the Data

```{r}
colnames(UN3)
summary(UN3)
glimpse(UN3)
```

1)From the summary analysis of the dataset, all the variables are quantitative and all the variables expect for Purban [ie. ModernC, Change, PPgdp, Frate, Pop, Fertility] have missing values.  



2)While calculating the mean and standard deviation of the quantitative predictors, we remove the NA values so that it does not affect measures of central tendency calculations.

```{r}
ModernC_mean = mean(UN3$ModernC, na.rm = TRUE) 
ModernC_std = sd(UN3$ModernC, na.rm = TRUE)
Change_mean = mean(UN3$Change, na.rm = TRUE)
Change_std = sd(UN3$Change, na.rm = TRUE)
PPgdp_mean = mean(UN3$PPgdp, na.rm = TRUE)
PPgdp_std = sd(UN3$PPgdp, na.rm = TRUE)
Frate_mean = mean(UN3$Frate, na.rm = TRUE)
Frate_std = sd(UN3$Frate, na.rm = TRUE)
Pop_mean = mean(UN3$Pop, na.rm = TRUE)
Pop_std = sd(UN3$Pop, na.rm = TRUE)
Fertility_mean = mean(UN3$Fertility, na.rm = TRUE)
Fertility_std = sd(UN3$Fertility, na.rm = TRUE)
Purban_mean = mean(UN3$Purban, na.rm = TRUE)
Purban_std = sd(UN3$Purban, na.rm = TRUE)

df=data.frame(matrix(nrow=0,ncol=3))
ModernC_df=c("ModernC",ModernC_mean,ModernC_std)
Change_df=c("Change",Change_mean,Change_std)
PPgdp_df=c("PPgdp",PPgdp_mean,PPgdp_std)
Frate_df=c("Frate",Frate_mean,Frate_std)
Pop_df=c("Pop",Pop_mean,Pop_std)
Fertility_df=c("Fertility",Fertility_mean,Fertility_std)
Purban_df=c("Purban",Purban_mean,Purban_std)

df=rbind(df,ModernC_df,Change_df,PPgdp_df,Frate_df,Pop_df,Fertility_df,Purban_df)

x=c("variable name","mean", "standard_deviation")
colnames(df)=x
kable(df)
  
```



3)The pair-plots here all remove missing data individually for each predictor. 

```{r message = FALSE, warning = FALSE}
ggpairs(UN3, columns=c(2,3,4,5,6,7,1))
```

From the pair-plots, we see that there is a high correlation between 'Fertility' and 'Change' (0.814), which can be intuitively explained (since high fertility leads to high propulation growth rate). There seem to be outlier points in 'Pop' and 'Purban'. 
The general distribution and scales of the predictors 'PPgdp' and 'Pop' seem to point towards taking their transformations. 
Plotting 'ModernC' as a dependent variable across the other predictors, the above mentioned transformations seem more pragmatic. The correlation values of 'ModernC' with 'Pop' and 'Frate' are relatively low and thus might not give much information about 'ModernC'.


## Model Fitting


4)
```{r}
modernc.lm=lm(ModernC ~ ., data = UN3)
summary(modernc.lm)
```

From the summary of the linear model displayed above; we understand that 85 observations were deleted due to missing values. Thus of the 210 data points only (210-85 = 125) 125 data points were used to fit the linear model. 

```{r}
par(mfrow=c(2,2))
plot(modernc.lm, ask=FALSE)
```

For the linear model, from the residual plots we (more or less) see that the residuals are having a mean of zero with some constant variance. While we do not observe a perfect straight line, the overall trend is more or less centered around 0. 

From the Q-Q plot, the assumption for ModernC being normally distributed doesnot completely hold true, since there seem to be a longer left tail(or a heavier right tail). 

There do not seem to be influential outliers since none of the points have a cook's distance >[0.5 or 1], but the points of 'India' and 'China' do seem to be outliers nonetheless. 

## Added Variable Plots

5) 

```{r}
car::avPlots(lm(ModernC ~ ., data = UN3))
```

From the added variables plots above, it is evident that PPgdp and Pop might be needing some kind of transformations. [This can be deciphered from the scale of the axis and the slope steepness of the ModernC vs the 'particular variable' regression line]. While the plots for the Change and the Fertility variables do show steepness, the scale of the variables (in their units) do not intent for any transformations.


## Transformations

6)

```{r}

car::boxTidwell(ModernC ~ Pop ,  ~ PPgdp + Change +  Frate + Fertility + Purban  , data = UN3 )
```
```{r}
car::boxTidwell(ModernC ~ Pop+PPgdp ,  ~ Change +  Frate + Fertility + Purban  , data = UN3 )

```

Running the boxTidwell plots for two cases (first with transformation only in the population variable and second with transformations for both the population and PPgdp variables), the second case seems to be more pragmatic in the approach (since in both the cases we would be taking the square root of the Pop variable) to variable transformation. {Taking the log of PPgdp variable can be understood by looking at the scale of the variable's units.}


7)

```{r}
# test1 = lm(ModernC ~ sqrt(Pop) + PPgdp + Change + Frate + Fertility + Purban, data = UN3)
test2 = lm(ModernC ~ sqrt(Pop) + log(PPgdp) + Change + Frate + Fertility + Purban, data = UN3)
car::boxCox(test2)

```

Plotting the boxCox plot for the response variable, with a reasonable confidence (of the 95% interval) it can be said that the response variable does not need any transformation as such. The 95% confidence interval leans to lambda = 1, which is easier to interpret rather than taking the absolute value of lambda. Taking the trade-off for interpretability over model accuracy, no transformation for the response variable would be needed.


## Model Fitting and Additional Transformations

8)

```{r}
test2.lm = lm(ModernC ~ sqrt(Pop) + log(PPgdp) + Change + Frate + Fertility + Purban, data = UN3)
summary(test2.lm)
```

```{r}
par(mfrow=c(2,2))
plot(test2.lm, ask=FALSE)
```
```{r}
car::avPlots(test2.lm)
```


With the new linear model fitted against the two transformed variables; the residual plots now are more aligned towards the general assumptions of a linear regression model. The residuals mean is now more centered towards zero and the variance in the residuals is relatively more equally distributed. The Normal distributions Q-Q plots still show some deviation from the normal distribution of the residuals { being heavier on the right tail}, but much better off than the un-transformed variable residuals. From the AVplots there still seems to be some transformation possibility for sqrt(Pop) but additional transformations might lead to a loss of interpretability of the model and thus the additional transformations are being avoided.   


9)

```{r}

test_.lm = lm(ModernC ~ sqrt(sqrt(Pop)) + log(PPgdp) + Change + Frate + Fertility + Purban, data = UN3)
summary(test_.lm)
par(mfrow=c(2,2))
plot(test_.lm, ask=FALSE)
car::avPlots(test_.lm)

```

An attempt to take an additional sqrt of the (sqrt(Pop)) variable was done and the model was tried to fit. With interpretability of the model being a questionable issue,  the residual plots of the fit model showed much deviation from a normal-distribution behaviour(along with a minor loss in the R squared value). This transformation is then ruled out as a worse model than the one deduced in question 8

Thus, after the various transformations of the predictors, the most pragmatic and interpretive friendly model is test2.lm = lm(ModernC ~ sqrt(Pop) + log(PPgdp) + Change + Frate + Fertility + Purban, data = UN3)


## Removal of Outliers

10)From the previous model analysis, the points pertaining to countries "India" and "China seem to be outliers. {They are not necessarily influential since their cook's distance is well below the threshold}

Re-analyzing the model by taking out the two rows pertaining to China (row 39) and India (row 86)

```{r}
UN3_outliers_removed = UN3[-c(39,86),]
```


```{r}
test3.lm = lm(ModernC ~ sqrt(Pop) + log(PPgdp) + Change + Frate + Fertility + Purban, data = UN3_outliers_removed)
summary(test3.lm)
```
```{r}
par(mfrow=c(2,2))
plot(test3.lm, ask=FALSE)
```


Refitting the model after removing the outliers, the residual plots now are more evenly distributed along the mean (0) value. While there is a minor decrease in the 'R-square' value (decrease in model's ability to explain variance), the trade-off towards developing a model that is closer towards the assumptions is acceptable. The residuals vs. Leverage plots also show even distribution around the mean (0) line with no evident outliers/influential points.  



## Summary of Results

11)


```{r}
kable(summary(test3.lm)$coeff, digits = c(3,3,3,3))
```

```{r}
kable(confint(test3.lm), digits = c(3,3))
```

From the regression model fit (post transformations and outlier removals), the interpretation of the coefficients can be summarized as below:-

For the predictor variable (y) in its originial units (% of unmarried women using a modern method of contraception),
-> An increase of population by 1000 members increase the % of unmarried women using modern methods of contraception{from now on referred to as just %} by (0.024)*1 = 0.024 % (approx. considering the base population levels are much, more higher than 1000 members) 
-> An increase of per capita GDP (2001, USD values) by x units, increase the % by  5.183 * log(base-value + x / base-value) 
-> An increase in annual population growth rate by 1% increases the % by 4.838%
-> An increase in females overage 15 that are economically active by 1% increases the % by 0.182%.
-> An increase in expected number of liver births per female by 1 unit decrease the % by 9.319%.
-> An increase in urban population by 1% is expected to decrease the % by 0.029%. 



## Model Summary Text

12)From the data collect by the UN containing the national health, welfare and education statistics of 210 places, only 125 data points (data from only 125 places) have complete entries that could be used to build a linear model. The model tries to explain the relationship between The % 
of unmarried women using modern methods of contraception to all the other macro economic factors viz. Annual population growth rate, Per capita GDP, place population, fertility of females and % of urban population. Two of the 125 data points(pertaining to India and China) have been removed from the model building exercise since they have extremely high population numbers which could result in wrong analysis. The model developed assumes that all the macro economic factors (with certain transformations for the factors of population and ppgdp) have a linear relationship with the predictor variable Y (% of women using modern methods of contraception). With this assumption, the model developed indicates that increase in population growth, per capita GDP, annual population growth rate, fertility rate; and decrease in Feritlity and urban population % increases the % of unmarries women using modern methods of contraception. {For exact changes and value estimates please see the interpretations of coeffients above}. 

The limitations of the model developed pertains to lack of causality. The model can not explain whether one factor(predictor variable) leads to a change in the ModernC variable or vice-versa. The model only explains correlations observed and does not explain any cause and effect relationships. 


## Methodology

    
13. Prove that the intercept in the added variable scatter plot will always be zero.  _Hint:  use the fact that if $H$ is the project matrix which contains a column of ones, then $1_n^T (I - H) = 0$.  Use this to show that the sample mean of residuals will always be zero if there is an intercept._

A) We know the premise that 

$$\hat{Y} = \hat{\beta_0} + \hat{\beta_1}.X$$

Thus applying for the errors in $Y$ and $X$, we get

$$ e_y = \hat{\beta_0} + \hat{\beta_1}.e_x $$

we know, 

$$ e_y=(I-H)Y $$
and $$ e_x = (I-H)X $$

Thus, we get,

$$ (I-H)Y=\hat{\beta_0}+\hat{\beta_1} (I-H)X $$

we know, 

$$ \hat{\beta_1} = (X'.X)^{-1} X'.Y $$

thus, 

$$ (I-H)Y=\hat{\beta_0}+(X'X)^{-1}X'Y  (I-H)X $$ 
we know,

$$ X = (I - H). X_j $$

simplifying further and rearranging the terms,

$$(I-H)Y=\hat{\beta_0}+(((I-H)X_j)'(I-H)X_j)^{-1}((I-H)X_j)'Y(I-H)(I-H)X_j $$
$$(I-H)Y=\hat{\beta}_0+(X_j'(I-H)'(I-H)X_j)^{-1}X_j'(I-H)'Y(I-H)(I-H)X_j$$
$$  (I-H)Y=\hat{\beta}_0+(X_j'(I-H)X_j)^{-1}X_j'(I-H)'Y(I-H)X_j $$

Note: We are using the property that $(I-H)$ is a diagonal and idempotent matrix.

Now, multiplying both sides of the equation with $(X_j)'$, we get,

$$ X_j'(I-H)Y=  X_j' \hat{\beta}_0 + X_j'(X_j'(I-H)X_j)^{-1} X_j'(I-H)'Y(I-H)X_j $$

$$ X_j'(I-H)Y=  X_j' \hat{\beta}_0 + X_j'(I-H)X_j(X_j'(I-H)X_j)^{-1} X_j'(I-H)Y $$
$$ X_j'(I-H)Y=  X_j' \hat{\beta}_0 + X_j'(I-H)Y $$
Thus,

$$ X_j' \hat{\beta}_0 = 0 $$ 

considering non-trivial cases, we conclude $\hat{\beta}_0 = 0$ thus, proving that the intercept value will always be zero.



14. For multiple regression with more than 2 predictors, say a full model given by `Y ~ X1 + X2 + ... Xp`   we create the added variable plot for variable `j` by regressing `Y` on all of the `X`'s except `Xj` to form `e_Y` and then regressing `Xj` on all of the other X's to form `e_X`.  Confirm that the slope in a manually constructed added variable plot for one of the predictors  in Ex. 10 is the same as the estimate from your model. 

A) We will use the models developed in Q)10 to confirm on the slopes. 

Let us take $X_j$ to be Change. 

```{r}
UN3_outliers_removed = UN3_outliers_removed[complete.cases(UN3_outliers_removed),]
e_Y = residuals(lm(ModernC ~ sqrt(Pop) + log(PPgdp) + Frate + Fertility + Purban, data = UN3_outliers_removed ))
e_X = residuals(lm(Change ~ sqrt(Pop) + log(PPgdp) + Frate + Fertility + Purban, data = UN3_outliers_removed))

linearmodel.lm = lm(e_Y ~ e_X)

coefficient_final_model = summary(test3.lm)$coefficients['Change',c('Estimate')]
coefficient_temp_model = summary(linearmodel.lm)$coefficients['e_X',c('Estimate')]


df_ = rbind(coefficient_final_model, coefficient_temp_model)

kable(df_)
```

We see that the slope for the manually constructed AV plot (with the added variable being 'Change') is same as the slope of the final model generated in Q.10, Thus emperically proving the ask in the question.
